{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 15831,
     "status": "error",
     "timestamp": 1737077903084,
     "user": {
      "displayName": "Fangzhong Xu",
      "userId": "09108404321047927904"
     },
     "user_tz": 300
    },
    "id": "DGuXCwQMEiRy",
    "outputId": "4b932d93-3bb3-4eb6-da3a-f82580b2b657"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gensim.models import word2vec\n",
    "import networkx as nx\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import xml.dom.minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "from GCN import *\n",
    "import community\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import *\n",
    "import csv\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "import copy\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# import tensorflow as tf2 #Tensorflow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EN8sO78FEiSD"
   },
   "outputs": [],
   "source": [
    "class AliasSampling:\n",
    "    def __init__(self, prob):\n",
    "        self.n = len(prob)\n",
    "        self.U = np.array(prob) * self.n\n",
    "        self.K = [i for i in range(len(prob))]\n",
    "        overfull, underfull = [], []\n",
    "        for i, U_i in enumerate(self.U):\n",
    "            if U_i > 1:\n",
    "                overfull.append(i)\n",
    "            elif U_i < 1:\n",
    "                underfull.append(i)\n",
    "        while len(overfull) and len(underfull):\n",
    "            i, j = overfull.pop(), underfull.pop()\n",
    "            self.K[j] = i\n",
    "            self.U[i] = self.U[i] - (1 - self.U[j])\n",
    "            if self.U[i] > 1:\n",
    "                overfull.append(i)\n",
    "            elif self.U[i] < 1:\n",
    "                underfull.append(i)\n",
    "\n",
    "    def sampling(self, n=1):\n",
    "        x = np.random.rand(n)\n",
    "        i = np.floor(self.n * x)\n",
    "        y = self.n * x - i\n",
    "        i = i.astype(np.int32)\n",
    "        res = [i[k] if y[k] < self.U[i[k]] else self.K[i[k]] for k in range(n)]\n",
    "        if n == 1:\n",
    "            return res[0]\n",
    "        else:\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xg7jVTcMEiSG"
   },
   "outputs": [],
   "source": [
    "def GHAC(mlist,G,idx_pid,n_clusters=-1):\n",
    "\n",
    "    distance=[]\n",
    "    graph=[]\n",
    "    for i in range(len(mlist)):\n",
    "        gtmp=[]\n",
    "        for j in range(len(mlist)):\n",
    "            if i<j and G.has_edge(idx_pid[i],idx_pid[j]):\n",
    "                cosdis=1/(1+np.exp(-np.dot(mlist[i],mlist[j])))\n",
    "                gtmp.append(cosdis)\n",
    "            elif i>j:\n",
    "                gtmp.append(graph[j][i])\n",
    "            else:\n",
    "                gtmp.append(0)\n",
    "        graph.append(gtmp)\n",
    "\n",
    "    graph=np.array(graph)\n",
    "    distance =np.multiply(graph,-1)\n",
    "\n",
    "    if n_clusters==-1:\n",
    "        best_m=-10000000\n",
    "\n",
    "        n_components, labels = connected_components(graph)\n",
    "        Gr=nx.from_numpy_matrix(graph)\n",
    "\n",
    "        graph[graph<=0.9]=0 #Edge pre-clustering\n",
    "        n_components1, labels = connected_components(graph)\n",
    "\n",
    "        for k in range(n_components1,n_components-1,-1):\n",
    "            model_HAC = AgglomerativeClustering(linkage=\"average\",affinity='precomputed',n_clusters=k)\n",
    "            model_HAC.fit(distance)\n",
    "            labels = model_HAC.labels_\n",
    "\n",
    "            part= {}\n",
    "            for j in range (len(labels)):\n",
    "                part[j]=labels[j]\n",
    "\n",
    "            mod = community.modularity(part,Gr)\n",
    "            if mod>=best_m:\n",
    "                best_m=mod\n",
    "                best_labels=labels\n",
    "        labels = best_labels\n",
    "    else:\n",
    "        model_HAC = AgglomerativeClustering(linkage=\"average\",affinity='precomputed',n_clusters=n_clusters)\n",
    "        model_HAC.fit(distance)\n",
    "        labels = model_HAC.labels_\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YrgrV7SMEiSI"
   },
   "outputs": [],
   "source": [
    "def pairwise_evaluate(correct_labels,pred_labels):\n",
    "    TP = 0.0  # Pairs Correctly Predicted To SameAuthor\n",
    "    TP_FP = 0.0  # Total Pairs Predicted To SameAuthor\n",
    "    TP_FN = 0.0  # Total Pairs To SameAuthor\n",
    "\n",
    "    for i in range(len(correct_labels)):\n",
    "        for j in range(i + 1, len(correct_labels)):\n",
    "            if correct_labels[i] == correct_labels[j]:\n",
    "                TP_FN += 1\n",
    "            if pred_labels[i] == pred_labels[j]:\n",
    "                TP_FP += 1\n",
    "            if (correct_labels[i] == correct_labels[j]) and (pred_labels[i] == pred_labels[j]):\n",
    "                TP += 1\n",
    "\n",
    "    if TP == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = TP / TP_FP\n",
    "        pairwise_recall = TP / TP_FN\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "    return pairwise_precision, pairwise_recall, pairwise_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wgoqfe_qEiSK"
   },
   "outputs": [],
   "source": [
    "# Pre-trained word2vec model\n",
    "# word2vec converts\n",
    "save_model_name = \"gene/word2vec.model\"\n",
    "model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "\n",
    "# This line defines a regular expression pattern r that matches one or more occurrences of a variety of punctuation and special characters.\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～]+'\n",
    "# This line defines a list of common stopwords. Stopwords are words that are often filtered out during text processing because they are considered to have little value in text analysis.\n",
    "stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','method','algrithom','by','model']\n",
    "# This line stems each word in the stopword list using the Porter stemmer. Stemming is the process of reducing a word to its root form. For example, 'using' might be reduced to 'use'.\n",
    "# porter_stemmer.stem(w) applies the stemming algorithm to each word w in the stopword list.\n",
    "# The list comprehension [porter_stemmer.stem(w) for w in stopword] creates a new list where each stopword has been stemmed.\n",
    "stopword = [porter_stemmer.stem(w) for w in stopword]\n",
    "\n",
    "# path is set to the directory containing the raw data files.\n",
    "path = \"raw-data/\"\n",
    "# file_names is a list of all filenames in the specified directory.\n",
    "file_names = os.listdir(path)\n",
    "\n",
    "# Starts a loop to process each file in file_names.\n",
    "for fname in file_names:\n",
    "    # Removes the last four characters from the filename, which is assumed to be the file extension (e.g., .xml).\n",
    "    fname = fname[:-4]\n",
    "    # Opens the file with the .xml extension in read mode with UTF-8 encoding and reads its content into the variable f.\n",
    "    f = open(path + fname + \".xml\",'r',encoding = 'utf-8').read()\n",
    "    # Replaces all occurrences of the & character in the file content with a space to clean the text.\n",
    "    text=re.sub(u\"&\",u\" \",f)\n",
    "    # Parses the cleaned XML text and gets the root element of the XML structure using the ElementTree (ET) module.\n",
    "    root = ET.fromstring(text)\n",
    "\n",
    "    correct_labels=[] # Initializes correct_labels as an empty list to store publication labels.\n",
    "    p_to={} # Initializes p_to as empty dictionaries to store the original words in the title of each publication.\n",
    "    p_t={}  # Initlizeis p_t as empty dictionaries to store stemmed words in the title of each publication, respectively.\n",
    "\n",
    "    # Loops through each <publication> element in the XML root.\n",
    "    for i in root.findall('publication'):\n",
    "        # Extracts the text content of the <id> child element of the current <publication> element and assigns it to pid.\n",
    "        pid = i.find('id').text\n",
    "\n",
    "        # Checks if pid is already in p_t dictionary. If it is, appends '1' to pid to ensure a unique identifier.\n",
    "        if pid in p_t:\n",
    "            pid = pid+'1'\n",
    "\n",
    "        # Extracts the text content of the <label> child element, converts it to an integer, and appends it to correct_labels.\n",
    "        correct_labels.append(int(i.find('label').text))\n",
    "\n",
    "        line = i.find('title').text # Extracts the text content of the <title> child element.\n",
    "        line = re.sub(r, ' ', line) # Replaces all characters matching the regular expression r (punctuation and special characters) with a space.\n",
    "        line = line.replace('\\t',' ') # Replaces all tab characters with a space.\n",
    "        line = line.lower() # Converts the title to lowercase.\n",
    "        split_cut = line.split(' ') # Splits the title into a list of words using spaces as delimiters.\n",
    "\n",
    "        p_t[pid]=[] # Initializes empty lists in p_t for the current pid.\n",
    "        p_to[pid]=[] # Initializes empty lists in p_to for the current pid.\n",
    "\n",
    "        # Iterates over each word j in the split title (split_cut)\n",
    "        for j in split_cut:\n",
    "            # If the length of the word is greater than 1, it is appended to the list in p_to.\n",
    "            if len(j)>1:\n",
    "                p_to[pid].append(j)\n",
    "                # If the stemmed version of the word is not in the stopword list, it is appended to the list in p_t.\n",
    "                if porter_stemmer.stem(j) not in stopword:\n",
    "                    p_t[pid].append(porter_stemmer.stem(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Gep5vdjEiSN",
    "outputId": "0543d07d-9918-427c-b1d5-b55067e228c1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 338\u001b[0m\n\u001b[0;32m    335\u001b[0m total_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(u_i)\u001b[38;5;241m/\u001b[39mbatch_size)\n\u001b[0;32m    336\u001b[0m display_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m--> 338\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGCN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_batch):\n",
      "File \u001b[1;32mc:\\Users\\lexue\\OneDrive\\Documents\\UROP\\Physician Scientists\\HGCN-name-disambiguation\\HGCN-name-disambiguation\\GCN.py:74\u001b[0m, in \u001b[0;36mGCN.__init__\u001b[1;34m(self, graph, node_features, first_layer_dim, embed_dim, batch_size)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(graph, node_features, embed_dim\u001b[38;5;241m=\u001b[39membed_dim, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m---> 74\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_default_graph\u001b[49m()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_layer_dim \u001b[38;5;241m=\u001b[39m first_layer_dim\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_params()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "# --------------------------Stemming Publication Titles-----------------------------\n",
    "\n",
    "# Pre-trained word2vec model\n",
    "# word2vec creates vector representation embeddings of text\n",
    "save_model_name = \"gene/word2vec.model\"\n",
    "model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "\n",
    "# This line defines a regular expression pattern r that matches one or more occurrences of a variety of punctuation and special characters.\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～]+'\n",
    "# This line defines a list of common stopwords. Stopwords are words that are often filtered out during text processing because they are considered to have little value in text analysis.\n",
    "stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','method','algrithom','by','model']\n",
    "# This line stems each word in the stopword list using the Porter stemmer. Stemming is the process of reducing a word to its root form. For example, 'using' might be reduced to 'use'.\n",
    "# porter_stemmer.stem(w) applies the stemming algorithm to each word w in the stopword list.\n",
    "# The list comprehension [porter_stemmer.stem(w) for w in stopword] creates a new list where each stopword has been stemmed.\n",
    "stopword = [porter_stemmer.stem(w) for w in stopword]\n",
    "\n",
    "# path is set to the directory containing the raw data files.\n",
    "path = \"raw-data/\"\n",
    "# file_names is a list of all filenames in the specified directory.\n",
    "file_names = os.listdir(path)\n",
    "\n",
    "# Starts a loop to process each file in file_names.\n",
    "for fname in file_names:\n",
    "    # Removes the last four characters from the filename, which is assumed to be the file extension (e.g., .xml).\n",
    "    fname = fname[:-4]\n",
    "    # Opens the file with the .xml extension in read mode with UTF-8 encoding and reads its content into the variable f.\n",
    "    f = open(path + fname + \".xml\",'r',encoding = 'utf-8').read()\n",
    "    # Replaces all occurrences of the & character in the file content with a space to clean the text.\n",
    "    text=re.sub(u\"&\",u\" \",f)\n",
    "    # Parses the cleaned XML text and gets the root element of the XML structure using the ElementTree (ET) module.\n",
    "    root = ET.fromstring(text)\n",
    "\n",
    "    correct_labels=[] # Initializes correct_labels as an empty list to store publication labels.\n",
    "    p_to={} # Initializes p_to as empty dictionaries to store the original words in the title of each publication.\n",
    "    p_t={}  # Initlizeis p_t as empty dictionaries to store stemmed words in the title of each publication, respectively.\n",
    "\n",
    "    # Loops through each <publication> element in the XML root.\n",
    "    for i in root.findall('publication'):\n",
    "        # Extracts the text content of the <id> child element of the current <publication> element and assigns it to pid.\n",
    "        pid = i.find('id').text\n",
    "\n",
    "        # Checks if pid is already in p_t dictionary. If it is, appends '1' to pid to ensure a unique identifier.\n",
    "        if pid in p_t:\n",
    "            pid = pid+'1'\n",
    "\n",
    "        # Extracts the text content of the <label> child element, converts it to an integer, and appends it to correct_labels.\n",
    "        correct_labels.append(int(i.find('label').text))\n",
    "\n",
    "        line = i.find('title').text # Extracts the text content of the <title> child element.\n",
    "        line = re.sub(r, ' ', line) # Replaces all characters matching the regular expression r (punctuation and special characters) with a space.\n",
    "        line = line.replace('\\t',' ') # Replaces all tab characters with a space.\n",
    "        line = line.lower() # Converts the title to lowercase.\n",
    "        split_cut = line.split(' ') # Splits the title into a list of words using spaces as delimiters.\n",
    "\n",
    "        p_t[pid]=[] # Initializes empty lists in p_t for the current pid.\n",
    "        p_to[pid]=[] # Initializes empty lists in p_to for the current pid.\n",
    "\n",
    "        # Iterates over each word j in the split title (split_cut)\n",
    "        for j in split_cut:\n",
    "            # If the length of the word is greater than 1, it is appended to the list in p_to.\n",
    "            if len(j)>1:\n",
    "                p_to[pid].append(j)\n",
    "                # If the stemmed version of the word is not in the stopword list, it is appended to the list in p_t.\n",
    "                if porter_stemmer.stem(j) not in stopword:\n",
    "                    p_t[pid].append(porter_stemmer.stem(j))\n",
    "\n",
    "# --------------------------Construct PHNet-----------------------------\n",
    "    pid_idx={} # Initializes an empty dictionary pid_idx to map publication IDs (pid) to unique indices.\n",
    "    idx_pid={} # Initializes an empty dictionary idx_pid to map unique indices back to publication IDs (pid).\n",
    "    idx=0 # Initializes a variable idx to keep track of the unique index for each publication.\n",
    "\n",
    "    # Creates an empty graph G using NetworkX, which will be used to construct the PHNet (Publication Heterogeneous Network).\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Starts a loop to iterate over each publication ID (pid) in the p_t dictionary. The p_t dictionary contains the processed and stemmed words for each publication ID.\n",
    "    for pid in p_t:\n",
    "        G.add_node(pid) # Adds the current publication ID (pid) as a node in the graph G.\n",
    "        pid_idx[pid]=idx # Maps the current publication ID (pid) to the current index (idx) in the pid_idx dictionary.\n",
    "        idx_pid[idx]=pid # Maps the current index (idx) back to the publication ID (pid) in the idx_pid dictionary.\n",
    "        idx=idx+1 # Increments the index idx by 1 to ensure the next publication ID gets a unique index.\n",
    "\n",
    "    # --------------------------Construct PHNet (CoAuthor Subgraph)-----------------------------\n",
    "\n",
    "    # This line initializes an empty undirected graph Ga using the NetworkX library. This graph will be used to represent the co-author relationships.\n",
    "    Ga = nx.Graph()\n",
    "    # This loop iterates over each publication ID (pid) in the dictionary p_t.\n",
    "    for pid in p_t:\n",
    "        # For each pid, it adds a node to the graph Ga. This ensures that every publication ID is represented as a node in the graph, even if it does not end up having any edges (co-author relationships).\n",
    "        Ga.add_node(pid)\n",
    "\n",
    "    # This line opens a text file containing author pairs and reads all its lines into the list fa. Each line in the file is expected to represent a pair of authors who have co-authored a paper together.\n",
    "    fa = open(\"experimental-results/authors/\" + fname + \"_authorlist.txt\",'r',encoding = 'utf-8').readlines()\n",
    "\n",
    "    # This loop iterates over each line in the list fa.\n",
    "    for line in fa:\n",
    "        # line.strip() removes any leading and trailing whitespace from the current line.\n",
    "        line.strip()\n",
    "\n",
    "        # line.split('\\t') splits the stripped line into components based on tab characters (\\t). The resulting list, split_cut, contains the different parts of the line.\n",
    "        split_cut = line.split('\\t')\n",
    "\n",
    "        # This line processes the first component of split_cut, which represents the index of the publication first author.\n",
    "        keyi = idx_pid[int(split_cut[0].strip())]\n",
    "        # It strips any whitespace from this component, converts it to an integer, and then uses this integer to look up the corresponding publication ID (keyi) in the idx_pid dictionary.\n",
    "        keyj = idx_pid[int(split_cut[1].strip())]\n",
    "\n",
    "        # Initializes a variable weights to 1, representing the initial weight of the edge between the two publications that share an author.\n",
    "        weights = 1\n",
    "        if Ga.has_edge(keyi,keyj):\n",
    "            # If the edge exists, increments the weight of the edge by 1.\n",
    "            Ga[keyi][keyj]['weight'] = Ga[keyi][keyj]['weight'] + weights\n",
    "        else:\n",
    "            # If the edge does not exist, adds a new edge between keyi and keyj with an initial weight of 1.\n",
    "            # Ga.add_edge(keyi,keyj,{'weight': weights})\n",
    "            Ga.add_edge(keyi, keyj, weight=weights)\n",
    "\n",
    "    # --------------------------Construct PHNet (CoVenue Subgraph)-----------------------------\n",
    "\n",
    "    # Initializes an empty graph Gv using NetworkX. This graph will be used to represent co-venue relationships between publications.\n",
    "    Gv = nx.Graph()\n",
    "\n",
    "    # Loops through each publication ID (pid) in the p_t dictionary.\n",
    "    for pid in p_t:\n",
    "        # Adds each pid as a node in the graph Gv. This ensures that every publication ID is represented as a node in the graph.\n",
    "        Gv.add_node(pid)\n",
    "\n",
    "    # Opens a file that contains pairs of publication IDs that occurred in the same conference or journal, reads all lines from this file, and stores them in the list fv.\n",
    "    fv = open(\"experimental-results/\" + fname + \"_jconfpair.txt\",'r',encoding = 'utf-8').readlines()\n",
    "\n",
    "    # Starts a loop to iterate over each line in the list fv.\n",
    "    for line in fv:\n",
    "        # Strips leading and trailing whitespace characters from the current line using line.strip(). This ensures clean data processing.\n",
    "        line.strip()\n",
    "        # Splits the current line into components using the tab character (\\t) as the delimiter, resulting in a list split_cut.\n",
    "        split_cut = line.split('\\t')\n",
    "\n",
    "        # Processes the first component of split_cut, which represents the index of the first publication ID. Strips any whitespace, converts it to an integer, and uses this integer to look up the corresponding publication ID (keyi) in the idx_pid dictionary.\n",
    "        keyi = idx_pid[int(split_cut[0].strip())]\n",
    "        # Processes the second component of split_cut, which represents the index of the second publication ID. Strips any whitespace, converts it to an integer, and uses this integer to look up the corresponding publication ID (keyj) in the idx_pid dictionary.\n",
    "        keyj = idx_pid[int(split_cut[1].strip())]\n",
    "        weights = 1\n",
    "        # Gv.add_edge(keyi,keyj,{'weight': weights})\n",
    "        # Adds an edge between keyi and keyj with an initial weight of 1 in the graph Gv.\n",
    "        Gv.add_edge(keyi, keyj, weight=weights)\n",
    "\n",
    "    # --------------------------Construct PHNet (CoTitle Subgraph)-----------------------------\n",
    "\n",
    "    # Initializes an empty graph Gt using NetworkX. This graph will be used to represent the co-title relationships between publications.\n",
    "    Gt = nx.Graph()\n",
    "    # Loops through each publication ID (pid) in the p_t dictionary.\n",
    "    for pid in p_t:\n",
    "        # Adds each pid as a node in the graph Gt. This ensures that every publication ID is represented as a node in the graph.\n",
    "        Gt.add_node(pid)\n",
    "\n",
    "    # These nested loops iterate over each pair of publication IDs (keyi and keyj) in the p_t dictionary.\n",
    "    for i, keyi in enumerate(p_t):\n",
    "        for j, keyj in enumerate(p_t):\n",
    "            # Converts the list of stemmed words for each publication (p_t[keyi] and p_t[keyj]) into sets.\n",
    "            # Computes the intersection of these two sets, which gives the common stemmed words between the two publications.\n",
    "            # The length of this intersection is assigned to the variable weights, representing the number of shared stemmed words in the titles of the two publications.\n",
    "            weights=len(set(p_t[keyi]).intersection(set(p_t[keyj])))\n",
    "            # Checks if the current index j is greater than the current index i to avoid duplicate edges and self-loops.\n",
    "            # Also checks if the weights (number of shared words) is greater than or equal to 2. This ensures that edges are only added for pairs of publications with at least two common words in their titles.\n",
    "            if (j>i and weights>=2):\n",
    "                # Gt.add_edge(keyi,keyj,{'weight': weights})\n",
    "                # Adds an edge between keyi and keyj in the graph Gt with the computed weight.\n",
    "                Gt.add_edge(keyi, keyj, weight=weights)\n",
    "\n",
    "\n",
    "    Glist=[]\n",
    "    Glist.append(Ga)\n",
    "    Glist.append(Gt)\n",
    "    Glist.append(Gv)\n",
    "\n",
    "    # Combine edges and weights from all graphs into the final graph G\n",
    "    for i in range(len(Glist)):\n",
    "        # For each graph Glist[i], iterates over its edges. The edges(data='weight') method returns tuples of the form (u, v, d), where u and v are the nodes connected by the edge, and d is the weight of the edge.\n",
    "        for u, v, d in Glist[i].edges(data='weight'):\n",
    "            if G.has_edge(u, v):\n",
    "                G[u][v]['weight'] += d  # If the edge already exists, it increments the weight of the edge in G by d, the weight from the current graph.\n",
    "            else:\n",
    "                G.add_edge(u, v, weight=d)  # If the edge does not exist in G, it adds a new edge between u and v with the weight d.\n",
    "    # Append the final combined graph G to the list Glist.\n",
    "    Glist.append(G)\n",
    "\n",
    "    # --------------------------Sampling Paths via Meta-path and Relation Weight Guided Random Walks:-----------------------------\n",
    "\n",
    "    all_neighbor_samplings=[] # Store sampling distributions for neighbor nodes\n",
    "    all_neg_sampling=[] # Store negative samples\n",
    "\n",
    "\n",
    "    # Starts a loop that iterates over each graph Gi in the list Glist. The variable i is the index of the graph.\n",
    "    for i,Gi in enumerate(Glist):\n",
    "        # Converts the graph Gi to a NumPy array representing its adjacency matrix. This matrix contains the edge weights between nodes.\n",
    "        adj_matrix = nx.to_numpy_array(Gi)\n",
    "        # Creates a deep copy of the graph Gi and stores it in Gtmp.\n",
    "        Gtmp= copy.deepcopy(Gi)\n",
    "        # Loops through each edge (u, v, d) in Gtmp, where u and v are nodes and d is the edge weight. Sets the weight of each edge to 1.\n",
    "        for u,v,d in Gtmp.edges(data = 'weight'):\n",
    "            Gtmp[u][v]['weight'] = 1\n",
    "        # Uses Dijkstra's algorithm to calculate the shortest path lengths between all pairs of nodes in Gtmp. The result is stored in the dictionary length.\n",
    "        length = dict(nx.all_pairs_dijkstra_path_length(Gtmp))\n",
    "\n",
    "        # Loops through each pair of nodes (u, v) in the length dictionary.\n",
    "        for u in length:\n",
    "            for v in length[u]:\n",
    "                # If there is no direct edge between u and v in Gtmp and the shortest path length between them is greater than 0, adds an edge with the shortest path length as the weight.\n",
    "                if Gtmp.has_edge(u,v) is False and length[u][v]>0:\n",
    "                    Gtmp.add_edge(u, v, weight=length[u][v])\n",
    "        # Converts the updated graph Gtmp to a NumPy array representing its adjacency matrix, where the weights represent shortest path lengths.\n",
    "        pathl_matrix = nx.to_numpy_array(Gtmp)\n",
    "\n",
    "        # Store sampling distributions for the current graph.\n",
    "        neighbor_samplings = []\n",
    "        neg_samplings=[]\n",
    "        # Loops through each node i in the graph\n",
    "        for i in range(G.number_of_nodes()):\n",
    "            # Gets the edge weights for node i from adj_matrix.\n",
    "            node_weights = adj_matrix[i]\n",
    "            # If the sum of weights is 0, appends 0 to neighbor_samplings.\n",
    "            if np.sum(node_weights)==0:\n",
    "                neighbor_samplings.append(0)\n",
    "            # Otherwise, normalizes the weights to create a probability distribution and appends an AliasSampling object initialized with this distribution to neighbor_samplings.\n",
    "            else :\n",
    "                weight_distribution = node_weights / np.sum(node_weights)\n",
    "                neighbor_samplings.append(AliasSampling(weight_distribution))\n",
    "\n",
    "            # Gets the shortest path lengths for node i from pathl_matrix.\n",
    "            node_i_degrees = pathl_matrix[i]\n",
    "            # Sets zero weights to 6, the weight for the node itself to 0, and any weights less than or equal to 1 to 0.\n",
    "            node_i_degrees[node_i_degrees==0] = 6\n",
    "            node_i_degrees[i]=0\n",
    "            node_i_degrees[node_i_degrees<=1] = 0\n",
    "\n",
    "            # If the sum of the adjusted weights is 0, appends 0 to neg_samplings.\n",
    "            if np.sum(node_i_degrees)==0:\n",
    "                neg_samplings.append(0)\n",
    "            # Otherwise, normalizes the weights to create a probability distribution and appends an AliasSampling object initialized with this distribution to neg_samplings.\n",
    "            else:\n",
    "                node_distribution = node_i_degrees / np.sum(node_i_degrees)\n",
    "                neg_samplings.append(AliasSampling(node_distribution))\n",
    "        # Appends the neighbor_samplings and neg_samplings lists to all_neighbor_samplings and all_neg_sampling, respectively.\n",
    "        all_neighbor_samplings.append(neighbor_samplings)\n",
    "        all_neg_sampling.append(neg_samplings)\n",
    "\n",
    "    numwalks=4 # Number of random walks to start from each node.\n",
    "    walklength=10 # Length of each random walk.\n",
    "    negative_num=3 # Number of negative samples to generate per positive sample.\n",
    "\n",
    "    # Initializes empty lists u_i, u_j, and label to store the node pairs and labels for training data.\n",
    "    u_i=[]\n",
    "    u_j=[]\n",
    "    label=[]\n",
    "    # Defines a metapath for the walks, specifying the sequence of graphs to use in the walks.\n",
    "    metapath=[0,1,0,2]\n",
    "\n",
    "    # Perform Random Walks\n",
    "\n",
    "    # Loops through each node node_index in the graph:\n",
    "    for node_index in range(G.number_of_nodes()):\n",
    "        # Starts numwalks random walks from the current node.\n",
    "        # For each walk, initializes the starting node and sets the graph index based on the metapath.\n",
    "        for j in range(0, numwalks):\n",
    "            node_start=node_index\n",
    "            g_index=j\n",
    "            gi=metapath[g_index]\n",
    "            # For each step in the walk\n",
    "            for i in range(0, walklength):\n",
    "                if all_neighbor_samplings[gi][node_start] != 0:\n",
    "                    # Samples a neighbor node from the current graph using the sampling distribution.\n",
    "                    node_p = all_neighbor_samplings[gi][node_start].sampling()\n",
    "                    u_i.append(node_start)\n",
    "                    u_j.append(node_p)\n",
    "                    # Records the pair (node_start, node_p) with a positive label (1).\n",
    "                    label.append(1)\n",
    "\n",
    "                    # Negative Sampling\n",
    "\n",
    "                    # Samples negative_num negative nodes and records the pairs (node_start, node_n) with negative labels (-1).\n",
    "                    if all_neg_sampling[-1][node_start] != 0:\n",
    "                        for k in range(negative_num):\n",
    "                            node_n = all_neg_sampling[-1][node_start].sampling()\n",
    "                            u_i.append(node_start)\n",
    "                            u_j.append(node_n)\n",
    "                            label.append(-1)\n",
    "\n",
    "                    # Updates the graph index and graph for the next step.\n",
    "                    g_index=(g_index+1)%len(metapath)\n",
    "                    gi=metapath[g_index]\n",
    "\n",
    "                    # Samples the next node and records the pair with a positive label (1).\n",
    "                    if all_neighbor_samplings[gi][node_p] != 0:\n",
    "\n",
    "                        node_p1 = all_neighbor_samplings[gi][node_p].sampling()\n",
    "                        u_i.append(node_start)\n",
    "                        u_j.append(node_p1)\n",
    "                        label.append(1)\n",
    "\n",
    "                        # Performs negative sampling as before.\n",
    "                        if all_neg_sampling[-1][node_start] != 0:\n",
    "                            for k in range(negative_num):\n",
    "                                node_n = all_neg_sampling[-1][node_start].sampling()\n",
    "                                u_i.append(node_start)\n",
    "                                u_j.append(node_n)\n",
    "                                label.append(-1)\n",
    "\n",
    "                    # Updates the starting node for the next step.\n",
    "                    node_start = node_p\n",
    "                # If no neighbors are available, performs negative sampling directly.\n",
    "                else:\n",
    "                    for k in range(negative_num):\n",
    "                        node_n = all_neg_sampling[-1][node_start].sampling()\n",
    "                        u_i.append(node_start)\n",
    "                        u_j.append(node_n)\n",
    "                        label.append(-1)\n",
    "                    # Updates the graph index and graph for the next step.\n",
    "                    g_index=(g_index+1)%len(metapath)\n",
    "                    gi=metapath[g_index]\n",
    "\n",
    "    # Training\n",
    "    node_attr = []\n",
    "    for pid in p_to:\n",
    "        words_vec=[]\n",
    "        for word in p_to[pid]:\n",
    "            # if (word in model_w):\n",
    "            if word in model_w.wv:\n",
    "                # words_vec.append(model_w[word])\n",
    "                 words_vec.append(model_w.wv[word])\n",
    "        if len(words_vec)==0:\n",
    "            words_vec.append(2*np.random.random(100)-1)\n",
    "        node_attr.append(np.mean(words_vec,0))\n",
    "    node_attr=np.array(node_attr)\n",
    "\n",
    "    batch_size = 64\n",
    "    total_batch = 3*int(len(u_i)/batch_size)\n",
    "    display_batch = 100\n",
    "\n",
    "    model = GCN(Glist, node_attr, batch_size=batch_size)\n",
    "\n",
    "    avg_loss = 0.\n",
    "    for i in range(total_batch):\n",
    "        sdx=(i*batch_size)%len(u_i)\n",
    "        edx=((i+1)*batch_size)%len(u_i)\n",
    "        #print (sdx,edx)\n",
    "        if edx>sdx:\n",
    "            u_ii = u_i[sdx:edx]\n",
    "            u_jj = u_j[sdx:edx]\n",
    "            labeli = label[sdx:edx]\n",
    "        else:\n",
    "            u_ii = u_i[sdx:]+u_i[0:edx]\n",
    "            u_jj = u_j[sdx:]+u_j[0:edx]\n",
    "            labeli = label[sdx:]+label[0:edx]\n",
    "        loss= model.train_line(u_ii, u_jj, labeli)\n",
    "        avg_loss += loss / display_batch\n",
    "        if i % display_batch == 0 and i > 0:\n",
    "            print ('%d/%d loss %8.6f' %(i,total_batch,avg_loss))\n",
    "            avg_loss = 0.\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluating\n",
    "    embed_matrix = model.cal_embed()\n",
    "    labels = GHAC(embed_matrix,Glist[-1],idx_pid,len(set(correct_labels)))\n",
    "    pairwise_precision, pairwise_recall, pairwise_f1 = pairwise_evaluate(correct_labels,labels)\n",
    "    result.append([fname,pairwise_precision, pairwise_recall, pairwise_f1])\n",
    "    print (correct_labels,len(set(correct_labels)))\n",
    "    print (list(labels),len(set(list(labels))))\n",
    "    print (fname,pairwise_precision, pairwise_recall, pairwise_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7_UhI6GEiSS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Macro-F1\n",
    "Prec = 0\n",
    "Rec = 0\n",
    "F1 = 0\n",
    "save_csvpath = 'result/'\n",
    "with open(save_csvpath+'AM_nok.csv','w',newline='',encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"name\",\"Prec\",\"Rec\",\"F1\"])\n",
    "    for i in result:\n",
    "        Prec = Prec + i[1]\n",
    "        Rec = Rec + i[2]\n",
    "        F1 = F1 + i[3]\n",
    "    Prec = Prec/len(result)\n",
    "    Rec = Rec/len(result)\n",
    "    F1 = F1/len(result)\n",
    "    writer.writerow([\"Avg\",Prec,Rec,F1])\n",
    "    for i in range(len(result)):\n",
    "        tmp = result[i]\n",
    "        writer.writerow(tmp[0:4])\n",
    "\n",
    "print (\"Avg\",Prec,Rec,F1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
